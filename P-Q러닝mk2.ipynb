{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "57iGsDCEhF0e"
   },
   "source": [
    "### 기본 아이디어 : Q러닝을 목표 18개로 나누어서 실행\n",
    "\n",
    "- Q러닝은 off-policy이므로 exploration / exploitation을 나눠 실행할 수 있다.\n",
    "\n",
    "- 각각의 목표에 따른 Q-Tables를 만들어서 학습시키고\n",
    "\n",
    " Q-Table에 따른 ε-Greedy 정책을 통해서 순차적 task를 수행\n",
    "\n",
    "- 빠른 exploration을 위해서 시작위치를 랜덤하게 정해줌  \n",
    "(단순히 랜덤이동이면 시작 위치에서 멀어질수록 탐색확률이 지수적으로 감소하기 때문)\n",
    "\\+ 시작점으로 수렴하는 Q-Table을 만들기 어려우니까"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 2198,
     "status": "ok",
     "timestamp": 1652858904903,
     "user": {
      "displayName": "장진구",
      "userId": "01664360836974338576"
     },
     "user_tz": -540
    },
    "id": "pJTUAd5y6jGD"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from string import ascii_uppercase\n",
    "from draw_utils_2 import * \n",
    "from pyglet.gl import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 382,
     "status": "ok",
     "timestamp": 1652858956644,
     "user": {
      "displayName": "장진구",
      "userId": "01664360836974338576"
     },
     "user_tz": -540
    },
    "id": "bvoMB3daC7yj",
    "outputId": "8ecff3bd-4a8c-4eca-fe79-54cf7212cf49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device_name = tf.test.gpu_device_name()\n",
    "print(device_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ogangza/chang_path_finding/path-finding-rl'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 488,
     "status": "ok",
     "timestamp": 1652859823331,
     "user": {
      "displayName": "장진구",
      "userId": "01664360836974338576"
     },
     "user_tz": -540
    },
    "id": "cFVLy7G2MsHv",
    "outputId": "e6a11ac6-67a7-4f0f-ac89-4537135377b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'T']\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "# reward\n",
    "move_reward = -1 \n",
    "obs_reward = -10     # 한 번 장애물에 부딪치면, 10번에 1번정도만 장애물쪽에 이동하도록 함\n",
    "goal_reward = 1000   # Q-value의 빠른 수렴을 위해 넉넉하게 리워드를 부여\n",
    "# print('reward:' , move_reward, obs_reward, goal_reward) \n",
    "inds = list(ascii_uppercase)[:17]  # 클래스 활용이 미숙해 전역변수로 변환\n",
    "inds.append(\"T\")     # T-state (최종 목적지=시작 목적지)를 추가해 줌 (box.csv에도 추가함)\n",
    "Q_table_name_lst = inds\n",
    "print(inds)\n",
    "Q_tables = [] #Q_table_A : np.zero((10, 9, 4)) 형태의 딕셔너리 혹은 Structure Array 형태로 만들어보고 싶었으나 실패\n",
    "for Q_table_name in Q_table_name_lst:\n",
    "    globals()[\"Q_table_{}\".format(Q_table_name)] = np.zeros((10, 9, 4)) \n",
    "    Q_tables.append(globals()[\"Q_table_{}\".format(Q_table_name)]) \n",
    "#일단 완전히 초기화된 Q-Table들의 리스트인 Q-Tables를 만듬, 그냥 np.zeros((18, 10, 9, 4)) 해도 됐을듯 ㅠㅠ\n",
    "print(len(Q_tables))\n",
    "\n",
    "local_path = '/home/ogangza/chang_path_finding/path-finding-rl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 953,
     "status": "ok",
     "timestamp": 1652859022269,
     "user": {
      "displayName": "장진구",
      "userId": "01664360836974338576"
     },
     "user_tz": -540
    },
    "id": "P2bTHCmp7zuf"
   },
   "outputs": [],
   "source": [
    "box_data = pd.read_csv(os.path.join(local_path, \"./data/box.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 386,
     "status": "ok",
     "timestamp": 1652859027752,
     "user": {
      "displayName": "장진구",
      "userId": "01664360836974338576"
     },
     "user_tz": -540
    },
    "id": "Tqy3Pj0s8OBm",
    "outputId": "163321c5-2069-42d6-9f61-4cb88e30bba6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5, 0], [4, 0], [3, 0], [2, 0], [0, 0], [0, 1], [0, 2], [0, 3], [0, 4], [0, 5], [0, 6], [0, 7], [0, 8], [2, 8], [3, 8], [4, 8], [5, 8], [9, 4]]\n"
     ]
    }
   ],
   "source": [
    "box_coor_index =[]\n",
    "for i in range(len(box_data)):\n",
    "    box_coor_index.append([box_data['row'][i], box_data['col'][i]])\n",
    "print(box_coor_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R6ZybKmCl7bf"
   },
   "source": [
    "### Q-Tables의 인덱스 = box_coor_index의 인덱스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 398,
     "status": "ok",
     "timestamp": 1652859031674,
     "user": {
      "displayName": "장진구",
      "userId": "01664360836974338576"
     },
     "user_tz": -540
    },
    "id": "Ai1dSnXfiJxH"
   },
   "outputs": [],
   "source": [
    "class Simulator_exploration:  #단순 탐색을 위한 환경\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        height : 그리드 높이\n",
    "        width : 그리드 너비 \n",
    "        inds : A ~ Q alphabet list\n",
    "        '''\n",
    "        # Load train data\n",
    "        self.files = pd.read_csv(os.path.join(local_path, \"./data/Q_Finder.csv\")) #각각 A부터 T까지 하나를 목표로하는 csv파일을 만듬\n",
    "        self.height = 10 #세로 10\n",
    "        self.width = 9 # 가로 9  #A~Q까지 알파벳(target) 선언\n",
    "        self.failcount = 0\n",
    "        self.successcount = 0\n",
    "    def set_box(self):\n",
    "        '''\n",
    "        아이템들이 있을 위치를 미리 정해놓고 그 위치 좌표들에 아이템이 들어올 수 있으므로 그리드에 100으로 표시한다.\n",
    "        데이터 파일에서 이번 에피소드 아이템 정보를 받아 가져와야 할 아이템이 있는 좌표만 -100으로 표시한다.\n",
    "        self.local_target에 에이전트가 이번에 방문해야할 좌표들을 저장한다.\n",
    "        따라서 가져와야하는 아이템 좌표와 end point 좌표(처음 시작했던 좌표로 돌아와야하므로)가 들어가게 된다.\n",
    "        '''\n",
    "        box_data = pd.read_csv(os.path.join(local_path, \"./data/box.csv\"))\n",
    "\n",
    "        # 물건이 들어있을 수 있는 경우\n",
    "        for box in box_data.itertuples(index = True, name ='Pandas'):  #판다스 데이터를 튜플로 iter해준 것 같다. 행/열 위치정보로 각 알파벳이 표시됨\n",
    "            self.grid[getattr(box, \"row\")][getattr(box, \"col\")] = 100   #아이템이 없는 경우 100\n",
    "\n",
    "        # 물건이 실제 들어있는 경우\n",
    "        order_item = list(set(inds) & set(self.items))\n",
    "        order_csv = box_data[box_data['item'].isin(order_item)]\n",
    "        for order_box in order_csv.itertuples(index = True, name ='Pandas'):\n",
    "            self.grid[getattr(order_box, \"row\")][getattr(order_box, \"col\")] = -100 # 아이템이 있는 경우 -100 <- 값이 지정되면 리턴으로 사용가능?\n",
    "            # local target에 가야 할 위치 좌표 넣기\n",
    "            self.local_target.append(\n",
    "                [getattr(order_box, \"row\"),\n",
    "                 getattr(order_box, \"col\")]\n",
    "                )# 타겟 위치 지정 완료\n",
    "            # print(order_box)\n",
    "            \n",
    "        # print(self.local_target)\n",
    "        # self.local_target.append([9,4])  # 마지막 위치를 추가해주지 않음\n",
    "\n",
    "        # 알파벳을 Grid에 넣어서 -> grid에 2Dconv 적용 가능\n",
    "\n",
    "    def set_obstacle(self):\n",
    "        '''\n",
    "        장애물이 있어야하는 위치는 미리 obstacles.csv에 정의되어 있다. 이 좌표들을 0으로 표시한다.\n",
    "        '''\n",
    "        obstacles_data = pd.read_csv(os.path.join(local_path, \"./data/obstacles.csv\"))\n",
    "        for obstacle in obstacles_data.itertuples(index = True, name ='Pandas'):\n",
    "            self.grid[getattr(obstacle, \"row\")][getattr(obstacle, \"col\")] = 0 #장애물 행,열 지정\n",
    "\n",
    "    def reset(self, epi):\n",
    "        '''\n",
    "        reset()은 첫 스텝에서 사용되며 그리드에서 에이전트 위치가 start point에 있게 한다.\n",
    "\n",
    "        :param epi: episode, 에피소드 마다 가져와야 할 아이템 리스트를 불러올 때 사용\n",
    "        :return: 초기셋팅 된 그리드\n",
    "        :rtype: numpy.ndarray\n",
    "        _____________________________________________________________________________________\n",
    "        items : 이번 에피소드에서 가져와야하는 아이템들\n",
    "        terminal_location : 현재 에이전트가 찾아가야하는 목적지\n",
    "        local_target : 한 에피소드에서 찾아가야하는 아이템 좌표, 마지막 엔드 포인트 등의 위치좌표들\n",
    "        actions: visualization을 위해 에이전트 action을 저장하는 리스트\n",
    "        curloc : 현재 위치\n",
    "        '''\n",
    "\n",
    "        # initial episode parameter setting\n",
    "        self.epi = epi\n",
    "        self.items = list(self.files.iloc[self.epi])[0]\n",
    "        self.cumulative_reward = 0\n",
    "        self.terminal_location = None\n",
    "        self.local_target = []\n",
    "        self.actions = [] # 정책에 따라 시간 순서대로 action 리스트에 append해주면 될듯\n",
    "\n",
    "        # initial grid setting\n",
    "        self.grid = np.ones((self.height, self.width), dtype=\"float16\") #초기는 전부0\n",
    "\n",
    "        # set information about the gridworld\n",
    "        self.set_box() #빈박스 + 타겟 박스\n",
    "        self.set_obstacle() #장애물\n",
    "\n",
    "        # start point를 grid에 표시\n",
    "        self.curloc = [np.random.randint(10), np.random.randint(9)] #시작 위치 초기화 (랜덤 위치)\n",
    "        # obstacle, target 등의 위치를 빼고 시작한다면 더 빠른 수렴이 가능하겠지만, 어차피 그 위치의 q-value는 사용될리 없으므로 완전 랜덤으로 해줌\n",
    "        self.grid[int(self.curloc[0])][int(self.curloc[1])] = -5       \n",
    "        \n",
    "        self.done = False\n",
    "        \n",
    "        return self.grid\n",
    "\n",
    "    def apply_action(self, action, cur_x, cur_y):\n",
    "        '''\n",
    "        에이전트가 행한 action대로 현 에이전트의 위치좌표를 바꾼다.\n",
    "        action은 discrete하며 4가지 up,down,left,right으로 정의된다.\n",
    "        \n",
    "        :param x: 에이전트의 현재 x 좌표\n",
    "        :param y: 에이전트의 현재 y 좌표\n",
    "        :return: action에 따라 변한 에이전트의 x 좌표, y 좌표\n",
    "        :rtype: int, int\n",
    "        '''\n",
    "        new_x = cur_x\n",
    "        new_y = cur_y\n",
    "        # up\n",
    "        if action == 0:\n",
    "            new_x = cur_x - 1\n",
    "        # down\n",
    "        elif action == 1:\n",
    "            new_x = cur_x + 1\n",
    "        # left\n",
    "        elif action == 2:\n",
    "            new_y = cur_y - 1\n",
    "        # right\n",
    "        else:\n",
    "            new_y = cur_y + 1\n",
    "\n",
    "        return int(new_x), int(new_y)\n",
    "\n",
    "\n",
    "    def get_reward(self, new_x, new_y, out_of_boundary):\n",
    "        '''\n",
    "        get_reward함수는 리워드를 계산하는 함수이며, 상황에 따라 에이전트가 action을 옳게 했는지 판단하는 지표가 된다.\n",
    "\n",
    "        :param new_x: action에 따른 에이전트 새로운 위치좌표 x\n",
    "        :param new_y: action에 따른 에이전트 새로운 위치좌표 y\n",
    "        :param out_of_boundary: 에이전트 위치가 그리드 밖이 되지 않도록 제한\n",
    "        :return: action에 따른 리워드\n",
    "        :rtype: float\n",
    "        '''\n",
    "\n",
    "        # 바깥으로 나가는 경우\n",
    "        if any(out_of_boundary):\n",
    "            reward = obs_reward\n",
    "                       \n",
    "        else:\n",
    "            # 장애물에 부딪히는 경우 \n",
    "            if self.grid[new_x][new_y] == 0:\n",
    "                reward = obs_reward  \n",
    "\n",
    "            # 현재 목표에 도달한 경우\n",
    "            elif new_x == self.terminal_location[0] and new_y == self.terminal_location[1]:\n",
    "                reward = goal_reward\n",
    "\n",
    "            # 그냥 움직이는 경우 \n",
    "            else:\n",
    "                reward = move_reward\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def step(self, action):\n",
    "        ''' \n",
    "        에이전트의 action에 따라 step을 진행한다.\n",
    "        action에 따라 에이전트 위치를 변환하고, action에 대해 리워드를 받고, 어느 상황에 에피소드가 종료되어야 하는지 등을 판단한다.\n",
    "        에이전트가 endpoint에 도착하면 gif로 에피소드에서 에이전트의 행동이 저장된다.\n",
    "\n",
    "        :param action: 에이전트 행동\n",
    "        :return:\n",
    "            grid, 그리드\n",
    "            reward, 리워드\n",
    "            cumulative_reward, 누적 리워드\n",
    "            done, 종료 여부\n",
    "            goal_ob_reward, goal까지 아이템을 모두 가지고 돌아오는 finish율 계산을 위한 파라미터\n",
    "\n",
    "        :rtype: numpy.ndarray, float, float, bool, bool/str\n",
    "\n",
    "        (Hint : 시작 위치 (9,4)에서 up말고 다른 action은 전부 장애물이므로 action을 고정하는 것이 좋음)\n",
    "        '''\n",
    "        # print(self.local_target)\n",
    "        self.terminal_location = self.local_target[0]\n",
    "        cur_x,cur_y = self.curloc\n",
    "        self.actions.append((cur_x, cur_y))\n",
    "\n",
    "        goal_ob_reward = False\n",
    "        \n",
    "        new_x, new_y = self.apply_action(action, cur_x, cur_y)\n",
    "\n",
    "        out_of_boundary = [new_x < 0, new_x >= self.height, new_y < 0, new_y >= self.width]\n",
    "\n",
    "        # 바깥으로 나가는 경우 종료\n",
    "        if any(out_of_boundary):\n",
    "            self.done = True\n",
    "            goal_ob_reward = True\n",
    "            self.failcount += 1 # 실패 카운터\n",
    "        else:\n",
    "            # 장애물에 부딪히는 경우 종료\n",
    "            if self.grid[new_x][new_y] == 0:\n",
    "                self.done = True\n",
    "                goal_ob_reward = True\n",
    "                self.failcount += 1 # 실패 카운터\n",
    "\n",
    "            # 현재 목표에 도달한 경우, 다음 목표설정\n",
    "            elif new_x == self.terminal_location[0] and new_y == self.terminal_location[1]:\n",
    "                self.successcount +=1 # 성공 카운터\n",
    "\n",
    "                # end point 일 때\n",
    "                # if [new_x, new_y] == [9,4]:\n",
    "                self.done = True # 목표에 도착하면 바로 done 수행\n",
    "\n",
    "                # self.local_target.remove(self.local_target[0]) \n",
    "                # self.grid[cur_x][cur_y] = 1\n",
    "                # self.grid[new_x][new_y] = -5\n",
    "                goal_ob_reward = True\n",
    "                self.curloc = [new_x, new_y]\n",
    "            else:\n",
    "                # 그냥 움직이는 경우 \n",
    "                self.grid[cur_x][cur_y] = 1\n",
    "                self.grid[new_x][new_y] = -5\n",
    "                self.curloc = [new_x,new_y]\n",
    "                \n",
    "        reward = self.get_reward(new_x, new_y, out_of_boundary)\n",
    "        self.cumulative_reward += reward\n",
    "\n",
    "        # if self.done == True:\n",
    "        #     if [new_x, new_y] == [9, 4]: #초기위치로 도달했을 때,\n",
    "        #         if self.terminal_location == [9, 4]: # 목표와 같을때만 gif로 만들어줌\n",
    "        #             # 완료되면 GIFS 저장\n",
    "        #             goal_ob_reward = 'finish'\n",
    "        #             height = 10\n",
    "        #             width = 9\n",
    "        #             display = Display(visible=False, size=(width, height))\n",
    "        #             display.start()\n",
    "        #\n",
    "        #             start_point = (9, 4)\n",
    "        #             unit = 50\n",
    "        #             screen_height = height * unit\n",
    "        #             screen_width = width * unit\n",
    "        #             log_path = \"./logs\"\n",
    "        #             data_path = \"./data\"\n",
    "        #             render_cls = Render(screen_width, screen_height, unit, start_point, data_path, log_path)\n",
    "        #             for idx, new_pos in enumerate(self.actions):\n",
    "        #                 render_cls.update_movement(new_pos, idx+1)\n",
    "        #\n",
    "        #             render_cls.save_gif(self.epi)\n",
    "        #             render_cls.viewer.close()\n",
    "        #             display.stop()\n",
    "        # 코랩환경이라 그림그리기는 주석처리함\n",
    "        return self.grid, reward, self.cumulative_reward, self.done, goal_ob_reward\n",
    "\n",
    "\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 455,
     "status": "ok",
     "timestamp": 1652859620839,
     "user": {
      "displayName": "장진구",
      "userId": "01664360836974338576"
     },
     "user_tz": -540
    },
    "id": "08XnWar5i0gM"
   },
   "outputs": [],
   "source": [
    "class QAgent_exploration():\n",
    "    def __init__(self):\n",
    "        self.eps = 0.0\n",
    "        self.eps_decay = 0.00\n",
    "    # 랜덤한 위치 -> 굳이 입실론을 통해 탐색하지 않아도 모든 지점에서 시작할 확률이 같으므로 입실론은 무의미\n",
    "    \n",
    "    def select_action(self, curloc, index):\n",
    "        # Q-table은 단순 그리디로 이동, 목표에 따라 Q-Tables에서 인덱스를 알아야하므로 인덱스를 인수로 추가해줌\n",
    "        x, y = curloc\n",
    "        coin = random.random()\n",
    "        if coin < self.eps:\n",
    "            action = random.randint(0,3)\n",
    "        else:\n",
    "            action_val = Q_tables[index][x,y,:] # 목표의 인덱스 = 목표의 Q-Table\n",
    "            action = np.argmax(action_val)\n",
    "        return action\n",
    "\n",
    "    def update_table(self, transition, index):\n",
    "        s, a, r , s_prime = transition\n",
    "        x, y = s\n",
    "        next_x, next_y = s_prime\n",
    "        a_prime = self.select_action(s_prime, index) \n",
    "        Q_tables[index][x,y,a] = Q_tables[index][x,y,a] + 0.1*(r + np.max(Q_tables[index][next_x, next_y, :]) - Q_tables[index][x,y,a])\n",
    "        #Q러닝 업데이트 식을 이용, 마찬가지로 index를 인수로 추가해 table에 접근함\n",
    "\n",
    "    def anneal_eps(self):\n",
    "        self.eps -= self.eps_decay\n",
    "        self.eps = max(self.eps, 0.0) # 만에하나 한번도 못가본 장소가 있을까봐 0.1만 뒀음 (0이라도 상관없을 듯)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 390,
     "status": "ok",
     "timestamp": 1652859846915,
     "user": {
      "displayName": "장진구",
      "userId": "01664360836974338576"
     },
     "user_tz": -540
    },
    "id": "KdJs-DQcvTus"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    env = Simulator_exploration()\n",
    "    agent = QAgent_exploration()\n",
    "    episode = 1500\n",
    "    files = pd.read_csv(os.path.join(local_path, \"./data/Q_Finder.csv\"))\n",
    "\n",
    "    for n_epi in range(episode): #에피소드 변수만큼 실행\n",
    "\n",
    "        for targetidx in range(len(files)): # 목표를 A~T까지 돌아가면서 수행 + 업데이트\n",
    "            items = list(files.iloc[targetidx])[0]\n",
    "            done = False\n",
    "            acount = 0\n",
    "            env.reset(targetidx)\n",
    "            while not done: # 한 에피소드가 끝날때 까지\n",
    "                s = env.curloc\n",
    "                a = agent.select_action(s, targetidx)\n",
    "                acount += 1\n",
    "                _, r, cum_reward, done, goal_ob_reward = env.step(a)\n",
    "                s_prime = env.curloc\n",
    "                agent.update_table((s, a, r, s_prime), targetidx)\n",
    "                s = s_prime\n",
    "            agent.anneal_eps()\n",
    "        if n_epi % 20 == 0:\n",
    "            print(f'에피소드 :{n_epi}, 액션 수: {acount}, 리턴: {cum_reward}, 성공율 : {(env.successcount/(env.successcount+env.failcount+0.00001)*100):2f} %')\n",
    "            env.successcount = 0\n",
    "            env.failcount = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 158852,
     "status": "error",
     "timestamp": 1652860010398,
     "user": {
      "displayName": "장진구",
      "userId": "01664360836974338576"
     },
     "user_tz": -540
    },
    "id": "W_g4nEXB-INI",
    "outputId": "acb1ebf5-1c19-43f7-d412-28f80be0e68c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "에피소드 :0, 액션 수: 9, 리턴: -18, 성공율 : 5.555552 %\n",
      "에피소드 :20, 액션 수: 1, 리턴: -10, 성공율 : 5.277778 %\n",
      "에피소드 :40, 액션 수: 4, 리턴: -13, 성공율 : 3.333333 %\n",
      "에피소드 :60, 액션 수: 1, 리턴: -10, 성공율 : 16.944444 %\n",
      "에피소드 :80, 액션 수: 58, 리턴: 943, 성공율 : 30.277777 %\n",
      "에피소드 :100, 액션 수: 3, 리턴: 998, 성공율 : 59.722221 %\n",
      "에피소드 :120, 액션 수: 1, 리턴: -10, 성공율 : 65.555554 %\n",
      "에피소드 :140, 액션 수: 2, 리턴: -11, 성공율 : 56.111110 %\n",
      "에피소드 :160, 액션 수: 5, 리턴: 996, 성공율 : 56.388887 %\n",
      "에피소드 :180, 액션 수: 7, 리턴: 994, 성공율 : 57.499998 %\n",
      "에피소드 :200, 액션 수: 6, 리턴: 995, 성공율 : 71.388887 %\n",
      "에피소드 :220, 액션 수: 1, 리턴: -10, 성공율 : 80.555553 %\n",
      "에피소드 :240, 액션 수: 4, 리턴: -13, 성공율 : 82.777775 %\n",
      "에피소드 :260, 액션 수: 5, 리턴: 996, 성공율 : 88.333331 %\n",
      "에피소드 :280, 액션 수: 2, 리턴: -11, 성공율 : 87.777775 %\n",
      "에피소드 :300, 액션 수: 9, 리턴: 992, 성공율 : 93.611109 %\n",
      "에피소드 :320, 액션 수: 1, 리턴: -10, 성공율 : 91.111109 %\n",
      "에피소드 :340, 액션 수: 7, 리턴: 994, 성공율 : 91.666664 %\n",
      "에피소드 :360, 액션 수: 3, 리턴: 998, 성공율 : 92.222220 %\n",
      "에피소드 :380, 액션 수: 4, 리턴: 997, 성공율 : 93.888886 %\n",
      "에피소드 :400, 액션 수: 3, 리턴: 998, 성공율 : 96.666664 %\n",
      "에피소드 :420, 액션 수: 4, 리턴: 997, 성공율 : 95.555553 %\n",
      "에피소드 :440, 액션 수: 12, 리턴: 989, 성공율 : 97.499997 %\n",
      "에피소드 :460, 액션 수: 10, 리턴: 991, 성공율 : 97.777775 %\n",
      "에피소드 :480, 액션 수: 5, 리턴: 996, 성공율 : 98.333331 %\n",
      "에피소드 :500, 액션 수: 6, 리턴: 995, 성공율 : 96.666664 %\n",
      "에피소드 :520, 액션 수: 8, 리턴: 993, 성공율 : 96.666664 %\n",
      "에피소드 :540, 액션 수: 2, 리턴: 999, 성공율 : 97.222220 %\n",
      "에피소드 :560, 액션 수: 13, 리턴: 988, 성공율 : 98.333331 %\n",
      "에피소드 :580, 액션 수: 7, 리턴: 994, 성공율 : 98.055553 %\n",
      "에피소드 :600, 액션 수: 10, 리턴: 991, 성공율 : 98.333331 %\n",
      "에피소드 :620, 액션 수: 8, 리턴: 993, 성공율 : 98.611108 %\n",
      "에피소드 :640, 액션 수: 12, 리턴: 989, 성공율 : 98.611108 %\n",
      "에피소드 :660, 액션 수: 10, 리턴: 991, 성공율 : 98.055553 %\n",
      "에피소드 :680, 액션 수: 4, 리턴: 997, 성공율 : 99.999997 %\n",
      "에피소드 :700, 액션 수: 8, 리턴: 993, 성공율 : 98.333331 %\n",
      "에피소드 :720, 액션 수: 7, 리턴: 994, 성공율 : 99.999997 %\n",
      "에피소드 :740, 액션 수: 7, 리턴: 994, 성공율 : 99.722219 %\n",
      "에피소드 :760, 액션 수: 6, 리턴: 995, 성공율 : 99.999997 %\n",
      "에피소드 :780, 액션 수: 8, 리턴: 993, 성공율 : 99.444442 %\n",
      "에피소드 :800, 액션 수: 9, 리턴: 992, 성공율 : 99.722219 %\n",
      "에피소드 :820, 액션 수: 2, 리턴: 999, 성공율 : 99.722219 %\n",
      "에피소드 :840, 액션 수: 13, 리턴: 988, 성공율 : 99.444442 %\n",
      "에피소드 :860, 액션 수: 7, 리턴: 994, 성공율 : 99.722219 %\n",
      "에피소드 :880, 액션 수: 8, 리턴: 993, 성공율 : 99.722219 %\n",
      "에피소드 :900, 액션 수: 11, 리턴: 990, 성공율 : 99.444442 %\n",
      "에피소드 :920, 액션 수: 8, 리턴: 993, 성공율 : 99.999997 %\n",
      "에피소드 :940, 액션 수: 13, 리턴: 988, 성공율 : 99.722219 %\n",
      "에피소드 :960, 액션 수: 6, 리턴: 995, 성공율 : 99.722219 %\n",
      "에피소드 :980, 액션 수: 9, 리턴: 992, 성공율 : 99.999997 %\n",
      "에피소드 :1000, 액션 수: 8, 리턴: 993, 성공율 : 99.999997 %\n",
      "에피소드 :1020, 액션 수: 12, 리턴: 989, 성공율 : 99.999997 %\n",
      "에피소드 :1040, 액션 수: 8, 리턴: 993, 성공율 : 99.999997 %\n",
      "에피소드 :1060, 액션 수: 5, 리턴: 996, 성공율 : 99.999997 %\n",
      "에피소드 :1080, 액션 수: 10, 리턴: 991, 성공율 : 99.999997 %\n",
      "에피소드 :1100, 액션 수: 2, 리턴: 999, 성공율 : 99.999997 %\n",
      "에피소드 :1120, 액션 수: 5, 리턴: 996, 성공율 : 99.999997 %\n",
      "에피소드 :1140, 액션 수: 7, 리턴: 994, 성공율 : 99.999997 %\n",
      "에피소드 :1160, 액션 수: 4, 리턴: 997, 성공율 : 99.999997 %\n",
      "에피소드 :1180, 액션 수: 5, 리턴: 996, 성공율 : 99.999997 %\n",
      "에피소드 :1200, 액션 수: 4, 리턴: 997, 성공율 : 99.999997 %\n",
      "에피소드 :1220, 액션 수: 5, 리턴: 996, 성공율 : 99.999997 %\n",
      "에피소드 :1240, 액션 수: 4, 리턴: 997, 성공율 : 99.999997 %\n",
      "에피소드 :1260, 액션 수: 12, 리턴: 989, 성공율 : 99.999997 %\n",
      "에피소드 :1280, 액션 수: 3, 리턴: 998, 성공율 : 99.999997 %\n",
      "에피소드 :1300, 액션 수: 7, 리턴: 994, 성공율 : 99.999997 %\n",
      "에피소드 :1320, 액션 수: 9, 리턴: 992, 성공율 : 99.999997 %\n",
      "에피소드 :1340, 액션 수: 13, 리턴: 988, 성공율 : 99.999997 %\n",
      "에피소드 :1360, 액션 수: 2, 리턴: 999, 성공율 : 99.999997 %\n",
      "에피소드 :1380, 액션 수: 10, 리턴: 991, 성공율 : 99.999997 %\n",
      "에피소드 :1400, 액션 수: 11, 리턴: 990, 성공율 : 99.999997 %\n",
      "에피소드 :1420, 액션 수: 11, 리턴: 990, 성공율 : 99.999997 %\n",
      "에피소드 :1440, 액션 수: 10, 리턴: 991, 성공율 : 99.999997 %\n",
      "에피소드 :1460, 액션 수: 14, 리턴: 987, 성공율 : 99.999997 %\n",
      "에피소드 :1480, 액션 수: 13, 리턴: 988, 성공율 : 99.999997 %\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/device:GPU:0'):\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 328,
     "status": "ok",
     "timestamp": 1652782012607,
     "user": {
      "displayName": "장진구",
      "userId": "01664360836974338576"
     },
     "user_tz": -540
    },
    "id": "nOub1PQUTBJu",
    "outputId": "f394b6fb-a0c2-42f4-c42e-3f734e68a87d"
   },
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# file = open(\"/content/drive/MyDrive/git_path_finding/path-finding-rl/data/save.txt\", \"wb\") \n",
    "# pickle.dump(Q_tables, file)\n",
    "# file.close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_p95zMujpn3F"
   },
   "source": [
    "### 텍스트로 Q테이블 저장해보려고 했는데 안됨... 알려주세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "4c8IbE_9rRo0"
   },
   "outputs": [],
   "source": [
    "# with open('Q_tables(20000).pickle', 'wb') as f:\n",
    "#     pickle.dump(Q_tables, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NNbmMtvGptQw"
   },
   "source": [
    "### 피클로 저장할 수 있다고 해서 저장 -> data 폴더에 넣어둠\n",
    "경로 지정은 어떻게 할지 모르겠습니다.\n",
    "\n",
    "cd 해서 폴더 위치로 오면 될지도?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Iun7MmBguXev"
   },
   "outputs": [],
   "source": [
    "# with open('Q_tables(20000).pickle','rb') as f:   \n",
    "#     Q_tables = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52uBSdoZp-4o"
   },
   "source": [
    "위치 찾아서 위 테이블을 로드하면 어디서나 사용가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 554,
     "status": "ok",
     "timestamp": 1652785137877,
     "user": {
      "displayName": "장진구",
      "userId": "01664360836974338576"
     },
     "user_tz": -540
    },
    "id": "nmOLwF51GHUd",
    "outputId": "e480d0b4-9c2a-48bc-9abd-f9913a52d7d9"
   },
   "outputs": [],
   "source": [
    "# print(Q_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dg9kwYKtqOGf"
   },
   "source": [
    "하드 코딩으로 Q-Tables를 저장해두고 필요하면 써보려 했으나 실패"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 398,
     "status": "ok",
     "timestamp": 1652863342059,
     "user": {
      "displayName": "장진구",
      "userId": "01664360836974338576"
     },
     "user_tz": -540
    },
    "id": "CglqAYjp2_3r"
   },
   "outputs": [],
   "source": [
    "class Simulator:\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        height : 그리드 높이\n",
    "        width : 그리드 너비 \n",
    "        inds : A ~ Q alphabet list\n",
    "        '''\n",
    "        # Load train data\n",
    "        self.files = pd.read_csv(os.path.join(local_path, \"./data/factory_order_test.csv\"))\n",
    "        self.height = 10 #세로 10\n",
    "        self.width = 9 # 가로 9  #A~Q까지 알파벳(target) 선언\n",
    "        self.failcount = 0\n",
    "        self.successcount = 0\n",
    "        self.init_local_target = []\n",
    "    def set_box(self):\n",
    "        '''\n",
    "        아이템들이 있을 위치를 미리 정해놓고 그 위치 좌표들에 아이템이 들어올 수 있으므로 그리드에 100으로 표시한다.\n",
    "        데이터 파일에서 이번 에피소드 아이템 정보를 받아 가져와야 할 아이템이 있는 좌표만 -100으로 표시한다.\n",
    "        self.local_target에 에이전트가 이번에 방문해야할 좌표들을 저장한다.\n",
    "        따라서 가져와야하는 아이템 좌표와 end point 좌표(처음 시작했던 좌표로 돌아와야하므로)가 들어가게 된다.\n",
    "        '''\n",
    "        box_data = pd.read_csv(os.path.join(local_path, \"./data/box.csv\"))\n",
    "\n",
    "        # 물건이 들어있을 수 있는 경우\n",
    "        for box in box_data.itertuples(index = True, name ='Pandas'):  #판다스 데이터를 튜플로 iter해준 것 같다. 행/열 위치정보로 각 알파벳이 표시됨\n",
    "            self.grid[getattr(box, \"row\")][getattr(box, \"col\")] = 100   #아이템이 없는 경우 100\n",
    "\n",
    "        # 물건이 실제 들어있는 경우\n",
    "        order_item = list(set(inds) & set(self.items))\n",
    "        order_csv = box_data[box_data['item'].isin(order_item)]\n",
    "        for order_box in order_csv.itertuples(index = True, name ='Pandas'):\n",
    "            self.grid[getattr(order_box, \"row\")][getattr(order_box, \"col\")] = -100 # 아이템이 있는 경우 -100 <- 값이 지정되면 리턴으로 사용가능?\n",
    "            # local target에 가야 할 위치 좌표 넣기\n",
    "            self.local_target.append(\n",
    "                [getattr(order_box, \"row\"),\n",
    "                 getattr(order_box, \"col\")]\n",
    "                )# 타겟 위치 지정 완료\n",
    "            # print(order_box)\n",
    "        # print(self.local_target)\n",
    "        self.local_target.append([9,4])  # 마지막 타겟 = 출발위치\n",
    "        self.init_local_target = copy.deepcopy(self.local_target)\n",
    "        \n",
    "        # 알파벳을 Grid에 넣어서 -> grid에 2Dconv 적용 가능\n",
    "\n",
    "    def set_obstacle(self):\n",
    "        '''\n",
    "        장애물이 있어야하는 위치는 미리 obstacles.csv에 정의되어 있다. 이 좌표들을 0으로 표시한다.\n",
    "        '''\n",
    "        obstacles_data = pd.read_csv(os.path.join(local_path, \"./data/obstacles.csv\"))\n",
    "        for obstacle in obstacles_data.itertuples(index = True, name ='Pandas'):\n",
    "            self.grid[getattr(obstacle, \"row\")][getattr(obstacle, \"col\")] = 0 #장애물 행,열 지정\n",
    "\n",
    "    def reset(self, epi):\n",
    "        '''\n",
    "        reset()은 첫 스텝에서 사용되며 그리드에서 에이전트 위치가 start point에 있게 한다.\n",
    "\n",
    "        :param epi: episode, 에피소드 마다 가져와야 할 아이템 리스트를 불러올 때 사용\n",
    "        :return: 초기셋팅 된 그리드\n",
    "        :rtype: numpy.ndarray\n",
    "        _____________________________________________________________________________________\n",
    "        items : 이번 에피소드에서 가져와야하는 아이템들\n",
    "        terminal_location : 현재 에이전트가 찾아가야하는 목적지\n",
    "        local_target : 한 에피소드에서 찾아가야하는 아이템 좌표, 마지막 엔드 포인트 등의 위치좌표들\n",
    "        actions: visualization을 위해 에이전트 action을 저장하는 리스트\n",
    "        curloc : 현재 위치\n",
    "        '''\n",
    "\n",
    "        # initial episode parameter setting\n",
    "        self.epi = epi\n",
    "        self.items = list(self.files.iloc[self.epi])[0]\n",
    "        self.cumulative_reward = 0\n",
    "        self.terminal_location = None\n",
    "        self.local_target = []\n",
    "        self.actions = [] # 정책에 따라 시간 순서대로 action 리스트에 append해주면 될듯\n",
    "\n",
    "\n",
    "        # initial grid setting\n",
    "        self.grid = np.ones((self.height, self.width), dtype=\"float16\") #초기는 전부0\n",
    "\n",
    "        # set information about the gridworld\n",
    "        self.set_box() #빈박스 + 타겟 박스\n",
    "        self.set_obstacle() #장애물\n",
    "\n",
    "        # start point를 grid에 표시\n",
    "        self.curloc = [9, 4] #시작 위치 초기화 (인덱스 0부터 시작이므로 10번째줄, 5번째 열임)\n",
    "        self.grid[int(self.curloc[0])][int(self.curloc[1])] = -5       \n",
    "        \n",
    "        self.done = False\n",
    "        \n",
    "        return self.grid\n",
    "\n",
    "    def apply_action(self, action, cur_x, cur_y):\n",
    "        '''\n",
    "        에이전트가 행한 action대로 현 에이전트의 위치좌표를 바꾼다.\n",
    "        action은 discrete하며 4가지 up,down,left,right으로 정의된다.\n",
    "        \n",
    "        :param x: 에이전트의 현재 x 좌표\n",
    "        :param y: 에이전트의 현재 y 좌표\n",
    "        :return: action에 따라 변한 에이전트의 x 좌표, y 좌표\n",
    "        :rtype: int, int\n",
    "        '''\n",
    "        new_x = cur_x\n",
    "        new_y = cur_y\n",
    "        # up\n",
    "        if action == 0:\n",
    "            new_x = cur_x - 1\n",
    "        # down\n",
    "        elif action == 1:\n",
    "            new_x = cur_x + 1\n",
    "        # left\n",
    "        elif action == 2:\n",
    "            new_y = cur_y - 1\n",
    "        # right\n",
    "        else:\n",
    "            new_y = cur_y + 1\n",
    "\n",
    "        return int(new_x), int(new_y)\n",
    "\n",
    "\n",
    "    def get_reward(self, new_x, new_y, out_of_boundary):\n",
    "        '''\n",
    "        get_reward함수는 리워드를 계산하는 함수이며, 상황에 따라 에이전트가 action을 옳게 했는지 판단하는 지표가 된다.\n",
    "\n",
    "        :param new_x: action에 따른 에이전트 새로운 위치좌표 x\n",
    "        :param new_y: action에 따른 에이전트 새로운 위치좌표 y\n",
    "        :param out_of_boundary: 에이전트 위치가 그리드 밖이 되지 않도록 제한\n",
    "        :return: action에 따른 리워드\n",
    "        :rtype: float\n",
    "        '''\n",
    "\n",
    "        # 바깥으로 나가는 경우\n",
    "        if any(out_of_boundary):\n",
    "            reward = obs_reward\n",
    "                       \n",
    "        else:\n",
    "            # 장애물에 부딪히는 경우 \n",
    "            if self.grid[new_x][new_y] == 0:\n",
    "                reward = obs_reward  \n",
    "\n",
    "            # 현재 목표에 도달한 경우\n",
    "            elif new_x == self.terminal_location[0] and new_y == self.terminal_location[1]:\n",
    "                reward = goal_reward\n",
    "\n",
    "            # 그냥 움직이는 경우 \n",
    "            else:\n",
    "                reward = move_reward\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def step(self, action):\n",
    "        ''' \n",
    "        에이전트의 action에 따라 step을 진행한다.\n",
    "        action에 따라 에이전트 위치를 변환하고, action에 대해 리워드를 받고, 어느 상황에 에피소드가 종료되어야 하는지 등을 판단한다.\n",
    "        에이전트가 endpoint에 도착하면 gif로 에피소드에서 에이전트의 행동이 저장된다.\n",
    "\n",
    "        :param action: 에이전트 행동\n",
    "        :return:\n",
    "            grid, 그리드\n",
    "            reward, 리워드\n",
    "            cumulative_reward, 누적 리워드\n",
    "            done, 종료 여부\n",
    "            goal_ob_reward, goal까지 아이템을 모두 가지고 돌아오는 finish율 계산을 위한 파라미터\n",
    "\n",
    "        :rtype: numpy.ndarray, float, float, bool, bool/str\n",
    "\n",
    "        (Hint : 시작 위치 (9,4)에서 up말고 다른 action은 전부 장애물이므로 action을 고정하는 것이 좋음)\n",
    "        '''\n",
    "        \n",
    "        ###\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        self.terminal_location = self.local_target[0]\n",
    "     \n",
    "        cur_x,cur_y = self.curloc\n",
    "        self.actions.append((cur_x, cur_y))\n",
    "\n",
    "        goal_ob_reward = False\n",
    "        \n",
    "        new_x, new_y = self.apply_action(action, cur_x, cur_y)\n",
    "\n",
    "        out_of_boundary = [new_x < 0, new_x >= self.height, new_y < 0, new_y >= self.width]\n",
    "\n",
    "        # 바깥으로 나가는 경우 종료\n",
    "        if any(out_of_boundary):\n",
    "            self.done = True\n",
    "            goal_ob_reward = True\n",
    "            self.failcount += 1\n",
    "            \n",
    "        else:\n",
    "            # 장애물에 부딪히는 경우 종료\n",
    "            if self.grid[new_x][new_y] == 0:\n",
    "                self.done = True\n",
    "                goal_ob_reward = True\n",
    "                self.failcount += 1\n",
    "               \n",
    "            # 현재 목표에 도달한 경우, 다음 목표설정\n",
    "            elif new_x == self.terminal_location[0] and new_y == self.terminal_location[1]:\n",
    "                \n",
    "\n",
    "                # end point 일 때\n",
    "                if [new_x, new_y] == [9,4]:\n",
    "                    self.done = True # 마지막 목표가 설정되면 done을 True로 설정\n",
    "                    self.successcount +=1\n",
    "\n",
    "                self.local_target.remove(self.local_target[0]) # 목표 달성시 목표 맨 앞 리스트를 지워준다.\n",
    "                self.grid[cur_x][cur_y] = 1\n",
    "                self.grid[new_x][new_y] = -5\n",
    "                goal_ob_reward = True\n",
    "                self.curloc = [new_x, new_y]\n",
    "            else:\n",
    "                # 그냥 움직이는 경우 \n",
    "                self.grid[cur_x][cur_y] = 1\n",
    "                self.grid[new_x][new_y] = -5\n",
    "                self.curloc = [new_x,new_y]\n",
    "               \n",
    "                \n",
    "        reward = self.get_reward(new_x, new_y, out_of_boundary)\n",
    "        self.cumulative_reward += reward\n",
    "#         print(\"local_taget\",self.local_target)\n",
    "#         print(\"terminal_location\",self.terminal_location)\n",
    "        if self.done == True:\n",
    "            if [new_x, new_y] == [9, 4]: #초기위치로 도달했을 때,\n",
    "                if self.terminal_location == [9, 4]: # 목표와 같을때만 gif로 만들어줌\n",
    "                    # 완료되면 GIFS 저장\n",
    "                    goal_ob_reward = 'finish'\n",
    "                    height = 10\n",
    "                    width = 9\n",
    "                    display = Display(visible=False, size=(width, height))\n",
    "                    display.start()\n",
    "                    start_point = (9, 4)\n",
    "                    unit = 50\n",
    "                    screen_height = height * unit\n",
    "                    screen_width = width * unit\n",
    "                    log_path = \"./logs\"\n",
    "                    data_path =\"./data\"\n",
    "                    print(self.init_local_target)\n",
    "                    render_cls = Render(self.init_local_target, screen_width, screen_height, unit, start_point, data_path, log_path)\n",
    "                    for idx, new_pos in enumerate(self.actions):\n",
    "                        render_cls.update_movement(new_pos, idx+1)\n",
    "        \n",
    "                    render_cls.save_gif(self.epi, '_성공')\n",
    "                    render_cls.viewer.close()\n",
    "                    display.stop()\n",
    "        else:\n",
    "                    \n",
    "                    # 완료되면 GIFS 저장\n",
    "                    goal_ob_reward = 'finish'\n",
    "                    height = 10\n",
    "                    width = 9\n",
    "                    display = Display(visible=False, size=(width, height))\n",
    "                    display.start()\n",
    "                    start_point = (9, 4)\n",
    "                    unit = 50\n",
    "                    screen_height = height * unit\n",
    "                    screen_width = width * unit\n",
    "                    log_path = \"./logs\"\n",
    "                    data_path =\"./data\"\n",
    "                    \n",
    "                    render_cls = Render(self.init_local_target, screen_width, screen_height, unit, start_point, data_path, log_path)\n",
    "                    for idx, new_pos in enumerate(self.actions):\n",
    "                        render_cls.update_movement(new_pos, idx+1)\n",
    "\n",
    "                    render_cls.save_gif(self.epi, '_실패')\n",
    "                    render_cls.viewer.close()\n",
    "                    display.stop()\n",
    "\n",
    "        return self.grid, reward, self.cumulative_reward, self.done, goal_ob_reward\n",
    "\n",
    "\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L2XXnRXAqr6c"
   },
   "source": [
    "환경은 원본코드 거의 그대로 사용, \n",
    "\n",
    "전역변수 선언해준 것\n",
    "\n",
    "factory_order_test만 불러옴\n",
    "\n",
    "사실 완성 Q-Table이 있으니 test나 train이나 그게 그거임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 377,
     "status": "ok",
     "timestamp": 1652863346610,
     "user": {
      "displayName": "장진구",
      "userId": "01664360836974338576"
     },
     "user_tz": -540
    },
    "id": "Jnjr9LD5Rsi9"
   },
   "outputs": [],
   "source": [
    "class QAgent_exploitation(): #테스트 클래스, 마찬가지로 입실론은 안써도 된다.\n",
    "    def __init__(self):\n",
    "        self.eps = 0.0  # exploitation만 하면됨\n",
    "        self.eps_decay = 0.00\n",
    "\n",
    "    \n",
    "    def select_action(self, curloc, index):\n",
    "        # index인수로 받아서 접근만 추가함\n",
    "        x, y = curloc\n",
    "        coin = random.random()\n",
    "        if coin < self.eps:\n",
    "            action = random.randint(0,3)\n",
    "        else:\n",
    "            action_val = Q_tables[index][x,y,:]\n",
    "            action = np.argmax(action_val)\n",
    "        return action\n",
    "\n",
    "    def update_table(self, transition, index):\n",
    "        s, a, r , s_prime = transition\n",
    "        x, y = s\n",
    "        next_x, next_y = s_prime\n",
    "        a_prime = self.select_action(s_prime, index) \n",
    "        # Q_tables[index][x,y,a] = Q_tables[index][x,y,a] + 0.1*(r + np.max(Q_tables[index][next_x, next_y, :]) - Q_tables[index][x,y,a])\n",
    "        # table은 더이상 업데이트 안해도됨, Q-Table만 따라 이동하면 된다.\n",
    "\n",
    "\n",
    "    def anneal_eps(self):\n",
    "        self.eps -= self.eps_decay\n",
    "        self.eps = max(self.eps, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 378,
     "status": "ok",
     "timestamp": 1652863351275,
     "user": {
      "displayName": "장진구",
      "userId": "01664360836974338576"
     },
     "user_tz": -540
    },
    "id": "YTfRP3B9tkYW"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    env = Simulator()\n",
    "    agent = QAgent_exploitation()\n",
    "    episode = len(env.files)\n",
    "\n",
    "    for n_epi in range(10):\n",
    "        alst = []\n",
    "        acount = 0\n",
    "        done = False\n",
    "        env.reset(n_epi)\n",
    "#         print(env.local_target)\n",
    "        while not done: # 에피소드가 끝날때 까지 반복\n",
    "            targetidx = box_coor_index.index(env.local_target[0])\n",
    "            s = env.curloc\n",
    "            a = agent.select_action(s, targetidx)\n",
    "            alst.append(a) #액션들을 alst에 모아둠 -> 나중에 시각화하려구\n",
    "            acount += 1 #액션 수\n",
    "            _, r, cum_reward, done, goal_ob_reward = env.step(a)\n",
    "            s_prime = env.curloc\n",
    "            agent.update_table((s, a, r, s_prime), targetidx)\n",
    "            s = s_prime\n",
    "        agent.anneal_eps()\n",
    "        print(f'에피소드 :{n_epi}, 액션 수: {acount}, 리턴: {cum_reward}, 성공율 : {(env.successcount/(env.successcount+env.failcount+0.00001)*100):2f} %')\n",
    "        if n_epi%20 ==0:\n",
    "            print(alst) #20회마다 액션리스트만 print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10252,
     "status": "ok",
     "timestamp": 1652863965404,
     "user": {
      "displayName": "장진구",
      "userId": "01664360836974338576"
     },
     "user_tz": -540
    },
    "id": "fMME8iBnxEFr",
    "outputId": "51bf04cf-b395-401e-84e8-ea5e596adff7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "[[5, 0], [4, 0], [2, 0], [0, 3], [0, 5], [0, 8], [4, 8], [9, 4]]\n",
      "go\n",
      "go1\n",
      "에피소드 :0, 액션 수: 40, 리턴: 7968, 성공율 : 99.999000 %\n",
      "[0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 3, 3, 3, 0, 1, 3, 3, 0, 3, 1, 3, 3, 0, 2, 1, 1, 3, 1, 1, 2, 1, 1, 1, 2, 2, 1, 2, 1]\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "[[0, 2], [0, 4], [0, 5], [0, 6], [0, 7], [0, 8], [5, 8], [9, 4]]\n",
      "go\n",
      "go1\n",
      "에피소드 :1, 액션 수: 34, 리턴: 7974, 성공율 : 99.999500 %\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "[[0, 3], [0, 6], [0, 7], [0, 8], [2, 8], [3, 8], [5, 8], [9, 4]]\n",
      "go\n",
      "go1\n",
      "에피소드 :2, 액션 수: 30, 리턴: 7978, 성공율 : 99.999667 %\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "[[5, 0], [2, 0], [0, 0], [0, 1], [0, 3], [0, 4], [0, 6], [9, 4]]\n",
      "go\n",
      "go1\n",
      "에피소드 :3, 액션 수: 38, 리턴: 7970, 성공율 : 99.999750 %\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "[[5, 0], [0, 2], [0, 3], [0, 7], [2, 8], [4, 8], [9, 4]]\n",
      "go\n",
      "go1\n",
      "에피소드 :4, 액션 수: 36, 리턴: 6971, 성공율 : 99.999800 %\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "[[5, 0], [0, 2], [0, 3], [0, 4], [0, 5], [0, 6], [5, 8], [9, 4]]\n",
      "go\n",
      "go1\n",
      "에피소드 :5, 액션 수: 40, 리턴: 7968, 성공율 : 99.999833 %\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "[[4, 0], [3, 0], [0, 0], [0, 4], [3, 8], [4, 8], [9, 4]]\n",
      "go\n",
      "go1\n",
      "에피소드 :6, 액션 수: 40, 리턴: 6967, 성공율 : 99.999857 %\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "[[5, 0], [4, 0], [3, 0], [0, 2], [0, 5], [0, 7], [2, 8], [9, 4]]\n",
      "go\n",
      "go1\n",
      "에피소드 :7, 액션 수: 36, 리턴: 7972, 성공율 : 99.999875 %\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "[[3, 0], [0, 0], [0, 1], [0, 5], [0, 6], [2, 8], [9, 4]]\n",
      "go\n",
      "go1\n",
      "에피소드 :8, 액션 수: 40, 리턴: 6967, 성공율 : 99.999889 %\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "go\n",
      "go1\n",
      "[[5, 0], [4, 0], [0, 2], [0, 5], [0, 7], [2, 8], [4, 8], [9, 4]]\n",
      "go\n",
      "go1\n",
      "에피소드 :9, 액션 수: 36, 리턴: 7972, 성공율 : 99.999900 %\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/device:GPU:0'):\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o9uc-TMzscn9"
   },
   "source": [
    "# 회고 + 더 나아가야 할 방향\n",
    "\n",
    "예상한대로 완전히 수렴한 여러 개의 Q-Table의 성능은 완벽했다.\n",
    "\n",
    "혹은 상태, 액션, 변수가 적다는 점에서 \n",
    "MDP를 아는 상황을 상정해 수렴하는 상태가치 table만 알 수 있어도\n",
    "더 빠른 최적 정책을 찾을 수 있을 것 같다.\n",
    "\n",
    "목표가 더 많아지는 경우 각각의 목표에 따라 Q-Table을 만들어 해결할 수 있을 것으로 예상되지만\n",
    "\n",
    "예측할 수 없는 변수들로 인해 전이 확률이 변화하거나 상태, 액션등이 많아서 테이블 전부 저장하기 힘들어진다면 단순 알고리즘보다 뉴럴넷을 사용하는 것이 연산량 대비 성능 비(?) 가 좋아지는 시점이 올 것으로 예상되지만 table기반 강화학습도 여전히 강력함을 확인 할 수 있었다.\n",
    "\n",
    "이런 경우 뉴럴넷을 연결한다면 분리해서 학습 후 취합하는 방식과 하나의 뉴럴넷으로 피쳐들 간에 관계들을 파악하는 것 중 어느 것이 더 좋을지 모르겠다.\n",
    "\n",
    "FC레이어들로 각각 학습시킨뒤에 선형결합으로 관계성을 파악하는 방식이 좋아보이는데...\n",
    "\n",
    "##추가 계획\n",
    "\n",
    "일단 일정 주기마다 업데이트한 Q-table을 validation set에 지속적으로 결과를 확인해보는 방식을 통해 적정 Q_table 수렴에 걸리는 episode와 수렴에 걸리는 시간, 사용된 램 용량 등을 체크해서 다른 분들의 모델과 비교해보는게 좋을 것 같다.\n",
    "\n",
    "GPU를 쓰지않아도 속도가 그렇게 느린건 아니었으나 \n",
    "정확한비교를 위해, GPU를 사용해서 학습시킬 수 있도록 해봐야할 것 같다.\n",
    "\n",
    "또한 임의의 목표 A, B, C를 따로 잡고 Q러닝을 학습하는 것이 아닌 train set으로 학습시켰을때의 차이도 궁금하다.\n",
    "\n",
    "휴리스틱 알고리즘 적용 : 좌수법에서 사용한 타겟 가져오기, 첫 행동 위로 고정 등\n",
    "\n",
    "## 궁금한 점들\n",
    "\n",
    "보상을 몇으로 설정해주는게 가장 학습이 빠른지 궁금하다.\n",
    "아마도... 선택할 액션의 숫자와 최적해(이동 수), grid의 가로, 세로 수에 따른 함수로 나타날 것으로 예상은 된다만,\n",
    "\n",
    "만약 Target의 위치가 랜덤하게 생성된다면? 모든 State에 대한 학습을 하면 되지 않을까?\n",
    "\n",
    "혹은 Target의 위치가 특정되지 않아 자유이동하는 물체라면 어떨까? 뜬구름 잡는건 말도안되니 5칸정도 이내에 trail을 남긴다면 어떤 방식으로 학습이 될까 궁금하다. 이런 경우라면 뉴럴넷이 더 유리할 수도 있을꺼같은데... "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOctIqqYmh/1KdCrQ37hXMg",
   "mount_file_id": "16zhJSSewXgPAFhwvoYncAGnwliQbLhqg",
   "name": "P-Q러닝.ipynb의 사본",
   "provenance": [
    {
     "file_id": "1Y4uwqambQWGcM7j07LtQRwvMeHbtZDhH",
     "timestamp": 1652852933489
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
